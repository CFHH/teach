# TEACH

## Official PyTorch implementation of the paper "TEACH: Temporal Action Compositions for 3D Humans" 

# TEACH: Temporal Action Compositions for 3D Humans [3DV-2022]
[![report](https://img.shields.io/badge/arxiv-report-red)](https://arxiv.org/abs/1912.05656)


<p float="center">
  <img src="assets/action2.gif" width="49%" />
  <img src="assets/action3.gif" width="49%" />
</p>


Check our YouTube videos below for more details.

### Video 

<!-- | Paper Video                                                                                                | Qualitative Results                                                                                                |
|------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------|
| [![PaperVideo](https://img.youtube.com/vi/rIr-nX63dUA/0.jpg)](https://www.youtube.com/watch?v=rIr-nX63dUA) | -->

## Features


This implementation:
- Instruction on how to prepare the datasets used in the experiments.
- The training code:
  - For both baselines
  - For TEACH method
- A simple interacting demo that given some prompts with texts and durations returns back:
  - a `npy` file containing the vertices of the body generated by TEACH.
  - a video that demonstrates the result.

## Updates

To be uploaded:
- Instructions about the baselines and how to run them.
- Instructions for sampling and evaluating with the code all of the models.
- The rendering code for the blender renderings used in the paper.

## Getting Started
TEACH has been implemented and tested on Ubuntu 20.04 with python >= 3.9.

Clone the repo:
```bash
git clone https://github.com/athn-nik/teach.git
```

After it do this to install DistillBERT:

```shell
cd deps/
git lfs install
git clone https://huggingface.co/distilbert-base-uncased
cd ..
```

Install the requirements using `virtualenv` :
```bash
# pip
source scripts/install_pip.sh
```
You can do something equivalent with `conda` as well.

## Running the Demo

We have prepared a nice demo code to run TEACH on arbitrary videos. 
First, you need download the required data(i.e our trained model from our [website](teach.is.tue.mpg.de)). 

Then, running the demo is as simple as:

```bash

# Run on your description
python interact_teach.py folder=/path/to/experiment output=/path/to/yourfname texts='[text prompt1, text prompt2, text prompt3, <more prompts comma divided>]' durs='[dur1, dur2, dur3, ...]'

```
## Data
Download the data from [AMASS website](amass.is.tue.mpg.de).

```shell
python divotion/dataset/process_amass.py --input-path /your path --output-path /out/path --model-type smplh --use-betas
```

Download the data from [BABEL website](babel.is.tue.mpg.de)[GET IT FROM ME]:

```shell
python divotion/dataset/add_babel_labels.py --input-path /is/cluster/nathanasiou/data/amass/processed_amass_smplh_wshape_30fps --out-path /is/cluster/nathanasiou/data/babel/babel-smplh30fps-gender --babel-path /is/cluster/nathanasiou/data/babel/babel_v2.1/
```

Softlink the data or copy them based on where you have them. You should have a data folder with the structure:
```
|-- amass
|   |-- processed_amass_smplh_wshape_30fps
|-- babel
|   |-- babel-smplh30fps-gender
|   |-- babel_v2.1
|-- smpl_models
|   |-- markers_mosh
|   |-- README.md
|   |-- smpl
|   |-- smplh
|   `-- smplx
```

Be careful not to push any data! To softlink your data, do:

`ln -s /path/to/data`

## Training
To start training after activating your environment. Do:

`python train.py experiment=baseline logger=none`

Explore `configs/train.yaml` to change some basic things like where you want
your output stored, which data you want to choose if you want to do a small
experiment on a subset of the data etc.
[TODO]: More on this coming soon.


## Citation

```bibtex
@inproceedings{athanasiou2022teach,
  title={TEACH: Temporal Action Compositions for 3D Humans},
  author={Athanasiou, Nikos and Petrovich, Mathis and Black, Michael J. and Varol, Gul},
  booktitle = {International Conference on 3D Vision (3DV)},
  month = {September},
  year = {2022}
}
```

## License
This code is available for **non-commercial scientific research purposes** as defined in the [LICENSE file](LICENSE). By downloading and using this code you agree to the terms in the [LICENSE](LICENSE). Third-party datasets and software are subject to their respective licenses.

## Acknowledgments
We thank [Benjamin Pellkofer](https://is.mpg.de/person/bpellkofer) for his IT support.

## References
Many part of this code were based on the official implementation of [TEMOS](https://github.com/Mathux/TEMOS). Here are some great resources we 
benefit:

- SMPL models and layer is from [SMPL-X model](https://github.com/vchoutas/smplx).
## Contact

This code repository was implemented mainly by [Nikos Athanasiou](https://is.mpg.de/~nathanasiou) with the help of [Mathis Petrovich](https://mathis.petrovich.fr/).

For commercial licensing (and all related questions for business applications), please contact ps-licensing@tue.mpg.de.
